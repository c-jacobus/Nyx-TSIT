base: &base

  # data
  num_data_workers: 16
  in_channels: [0, 1, 2, 3, 17, 18, 19] # u10, v10, t2m, sp, r500, r850, tcwv
  out_channels: [0] # tp
  
  min_path: '/pscratch/sd/s/shas1693/data/era5/mins.npy'
  max_path: '/pscratch/sd/s/shas1693/data/era5/maxs.npy'
  time_means_path:   '/pscratch/sd/s/shas1693/data/era5/time_means.npy'
  global_means_path: '/pscratch/sd/s/shas1693/data/era5/global_means.npy'
  global_stds_path:  '/pscratch/sd/s/shas1693/data/era5/global_stds.npy'
  precip: '/pscratch/sd/p/pharring/ERA5/precip/total_precipitation'
  precip_eps: 1e-5 # epsilon for normalizing precip log(1+tp/eps)
  dt: 1 # timestep length
  n_history: 0 # history input
  crop_size_x: None # crop height of input
  crop_size_y: None # crop width of input
  roll: False 
  conttime: False
  normalization: 'zscore'
  add_grid: False

  # model types
  model: 'pix2pix' # which model to use
  norm_G: 'spectralfadebatch3x3' # instance normalization or batch normalization for generator
  norm_D: 'spectralinstance' # instance normalization or batch normalization for discriminator
  norm_S: 'spectralinstance' # instance normalization or batch normalization for style stream
  norm_E: 'spectralinstance' # instance normalization or batch normalization for auxiliary encoder

  # input/output sizes
  batch_size: 16 # input batch size
  img_size: [512, 512, 512] # w,h of input image
  box_size: [512, 512, 512] # total size of simulation boxes (train, validation) 
  data_size: 128 # size of crops for training
  input_nc: 5 # number of input label classes (aka num input channels)
  output_nc: 5 # number of output image channels

  # for generator
  netG: 'tsit' # selects model architecture for generator (tsit | pix2pixhd)
  ngf: 64 # number of gen filters in first conv layer
  init_type: 'xavier' # network initialization [normal|xavier|kaiming|orthogonal]
  init_variance: 0.02 # variance of the initialization distribution
  z_dim: 256 # dimension of the latent z vector
  alpha: 1. # The parameter that controls the degree of stylization (between 0 and 1)
  no_ss: True # discard the style stream (not needed for standard SIS)
  downsamp: True # Start from downsampled content rather than random noise vector.
  num_upsampling_blocks: 6 # number of upsampling blocks (aka spatial scales) in generator
  use_periodic_padding: True # whether or not to use periodic padding along horizontal direction
  additive_noise: False # whether or not to use additional additive noise at each G stage

  # VAE setup
  nef: 16 # number of encoder filters in the first conv layer
  use_vae: False # enable training with an image encoder.

  # for training
  niter: 20 # number of iter at starting learning rate. This is NOT the total #epochs. Totla #epochs is niter + niter_decay
  niter_decay: 20 # number of iters to linearly decay learning rate to zero
  optimizer: 'adam' # optimizer type 
  beta1: 0.5 # momentum term of adam
  beta2: 0.999 # momentum term of adam
  lr: 2E-4 # initial learning rate for adam
  D_steps_per_G: 1 # number of discriminator iterations per generator iterations
  use_spec_loss: False # if specified, use spectral loss term forG 
  lambda_spec: 1.  # spectral loss term weight
  use_l1_loss: False # if specified, use l1 loss term forG 
  lambda_l1: 1.  # l1 loss term weight

  # for discriminators
  ndf: 64 # number of discrim filters in first conv layer
  lambda_feat: 0.5 # weight for feature matching loss
  no_ganFeat_loss: False # if specified, do *not* use discriminator feature matching loss
  gan_mode: 'hinge' # GAN loss function [ls|original|hinge]
  netD: 'multiscale' #  selects model architecture for discriminator multiscale|nlayer
  num_D: 2 # number of discriminators in multiscale-discriminator setup (was 2)
  n_layers_D: 4 # nubmer of layers in each discriminator (was 4)
  no_TTUR: True # if specified, do not use two-timescale update rule training scheme
  lambda_kld: 0.05 # KL divergence term weight
  cat_inp: False # concat input image to discriminator

  # Logging / Weights & Biases
  entity: 'weatherbenching'
  project: 'Nyx-TSIT'
  log_to_wandb: True
  log_to_screen: True
  save_checkpoint: True
  
  # Legacy config
  weight_init: {conv_init: 'normal', conv_scale: 0.02, conv_bias: 0.}
  lambda_rho: 0 # weight for additional rho loss term
  full_scale: True # whether or not to use all 6 of the scales in U-Net
  global_batch_size: 64 # number of samples per training batch
  base_batch_size: 64 # single GPU batch size
  Nsamples: 1024
  Nsamples_val: 128
  num_epochs: 10
  enable_amp: False
  enable_apex: False
  enable_benchy: False
  enable_jit: False
  expdir: '/logs'

  # params for setting learning rate (cosine decay schedule):
  #   start_lr: initial learning rate
  #   end_lr:  final learning rate
  #   warmup_steps: number of steps over which to do linear warm-up of learning rate
  #                 *not used when training single-GPU or when scaling='none'*
  #   scaling: 'none' initial lr doesn't change with respect to global batch size
  #            'linear' scale up according to lr_global_batchsize = (global_batchsize/base_batchsize)*lr_base_batchsize
  #            'sqrt' scale up according to lr_global_batchsize = sqrt(global_batchsize/base_batchsize)*lr_base_batchsize
  lr_schedule: {scaling: 'sqrt', start_lr: 2.E-4, end_lr: 0., warmup_steps: 128}

  # Data
  data_loader_config: 'lowmem' # choices: 'synthetic', 'inmem', 'lowmem', 'dali-lowmem'
  
  N_out_channels: 5
  N_in_channels: 5
  # HDF5 files for PyTorch native dataloader
  train_path: '/pscratch/sd/c/cjacobus/Nyx_512/train_s1_512_hydro.h5'
  val_path: '/pscratch/sd/c/cjacobus/Nyx_512/train_s1_512_hydro.h5'
  use_cache: None # set this to a cache dir (e.g., NVMe on CoriGPU) if you copied data there


bs64_nopad:
  <<: *base
  batch_size: 64
  use_periodic_padding: False
  norm_G: 'fadebatch3x3'

bs64_spec:
  <<: *base
  batch_size: 64
  use_periodic_padding: True
  norm_G: 'spectralfadebatch3x3'

bs64:
  <<: *base
  batch_size: 64
  use_periodic_padding: True
  norm_G: 'fadebatch3x3'

bs64_nopad_spec:
  <<: *base
  batch_size: 64
  use_periodic_padding: False
  norm_G: 'spectralfadebatch3x3'

bs64_cat_spec:
  <<: *base
  batch_size: 64
  use_periodic_padding: True
  norm_G: 'spectralfadebatch3x3'
  cat_inp: True

bs64_cat:
  <<: *base
  batch_size: 64
  use_periodic_padding: True
  norm_G: 'fadebatch3x3'
  cat_inp: True

