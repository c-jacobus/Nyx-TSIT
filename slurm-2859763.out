+ srun -u shifter -V /pscratch/sd/c/cjacobus/ML_Hydro_train/logs:/logs --image=nersc/pytorch:ngc-22.03-v0 --env PYTHONUSERBASE=/global/homes/c/cjacobus/.local/perlmutter/nersc-pytorch-ngc-22.03-v0 bash -c '
    source export_DDP_vars.sh
    python train.py --root_dir=/pscratch/sd/c/cjacobus/tsit --config=tighttrain_up5_tanh_L1_spec
    '
2022-08-01 08:33:09,957 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-01 08:33:09,960 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-01 08:33:09,963 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-01 08:33:10,246 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 10
2022-08-01 08:33:10,248 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 11
2022-08-01 08:33:10,251 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 8
2022-08-01 08:33:10,252 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 9
2022-08-01 08:33:11,092 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-01 08:33:11,205 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 13
2022-08-01 08:33:11,207 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 12
2022-08-01 08:33:11,210 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 14
2022-08-01 08:33:11,211 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 15
2022-08-01 08:33:11,253 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 7
2022-08-01 08:33:11,259 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 6
2022-08-01 08:33:11,260 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 5
2022-08-01 08:33:11,260 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 4
2022-08-01 08:33:11,261 - torch.distributed.distributed_c10d - INFO - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,261 - torch.distributed.distributed_c10d - INFO - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,262 - torch.distributed.distributed_c10d - INFO - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,263 - torch.distributed.distributed_c10d - INFO - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,263 - torch.distributed.distributed_c10d - INFO - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,263 - torch.distributed.distributed_c10d - INFO - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,264 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,264 - torch.distributed.distributed_c10d - INFO - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,265 - torch.distributed.distributed_c10d - INFO - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,266 - torch.distributed.distributed_c10d - INFO - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,267 - torch.distributed.distributed_c10d - INFO - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,267 - torch.distributed.distributed_c10d - INFO - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,268 - torch.distributed.distributed_c10d - INFO - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,269 - torch.distributed.distributed_c10d - INFO - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,270 - torch.distributed.distributed_c10d - INFO - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,270 - torch.distributed.distributed_c10d - INFO - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2022-08-01 08:33:11,282 - root - INFO - ------------------ Configuration ------------------
2022-08-01 08:33:11,282 - root - INFO - Configuration file: /global/u1/c/cjacobus/Nyx-TSIT/config/tsit.yaml
2022-08-01 08:33:11,282 - root - INFO - Configuration name: tighttrain_up5_tanh_L1_spec
2022-08-01 08:33:11,282 - root - INFO - Nsamples 8192
2022-08-01 08:33:11,282 - root - INFO - Nsamples_val 512
2022-08-01 08:33:11,282 - root - INFO - niter 200
2022-08-01 08:33:11,282 - root - INFO - niter_decay 150
2022-08-01 08:33:11,282 - root - INFO - use_spec_loss False
2022-08-01 08:33:11,282 - root - INFO - lambda_spec 100.0
2022-08-01 08:33:11,282 - root - INFO - use_l1_loss True
2022-08-01 08:33:11,282 - root - INFO - lambda_l1 300.0
2022-08-01 08:33:11,282 - root - INFO - data_size 128
2022-08-01 08:33:11,282 - root - INFO - train_path /pscratch/sd/c/cjacobus/Nyx_512/train_s1_512_invar3_fullnorm_hydro.h5
2022-08-01 08:33:11,282 - root - INFO - val_path /pscratch/sd/c/cjacobus/Nyx_512/train_s1_512_invar3_fullnorm_hydro.h5
2022-08-01 08:33:11,282 - root - INFO - num_upsampling_blocks 5
2022-08-01 08:33:11,282 - root - INFO - tanh False
2022-08-01 08:33:11,282 - root - INFO - num_data_workers 16
2022-08-01 08:33:11,282 - root - INFO - in_channels [0, 1, 2, 3, 17, 18, 19]
2022-08-01 08:33:11,282 - root - INFO - out_channels [0]
2022-08-01 08:33:11,282 - root - INFO - min_path /pscratch/sd/s/shas1693/data/era5/mins.npy
2022-08-01 08:33:11,282 - root - INFO - max_path /pscratch/sd/s/shas1693/data/era5/maxs.npy
2022-08-01 08:33:11,282 - root - INFO - time_means_path /pscratch/sd/s/shas1693/data/era5/time_means.npy
2022-08-01 08:33:11,282 - root - INFO - global_means_path /pscratch/sd/s/shas1693/data/era5/global_means.npy
2022-08-01 08:33:11,282 - root - INFO - global_stds_path /pscratch/sd/s/shas1693/data/era5/global_stds.npy
2022-08-01 08:33:11,282 - root - INFO - precip /pscratch/sd/p/pharring/ERA5/precip/total_precipitation
2022-08-01 08:33:11,283 - root - INFO - precip_eps 1e-05
2022-08-01 08:33:11,283 - root - INFO - dt 1
2022-08-01 08:33:11,283 - root - INFO - n_history 0
2022-08-01 08:33:11,283 - root - INFO - crop_size_x None
2022-08-01 08:33:11,283 - root - INFO - crop_size_y None
2022-08-01 08:33:11,283 - root - INFO - roll False
2022-08-01 08:33:11,283 - root - INFO - conttime False
2022-08-01 08:33:11,283 - root - INFO - normalization zscore
2022-08-01 08:33:11,283 - root - INFO - add_grid False
2022-08-01 08:33:11,283 - root - INFO - model pix2pix
2022-08-01 08:33:11,283 - root - INFO - norm_G spectralfadebatch3x3
2022-08-01 08:33:11,283 - root - INFO - norm_D spectralinstance
2022-08-01 08:33:11,283 - root - INFO - norm_S spectralinstance
2022-08-01 08:33:11,283 - root - INFO - norm_E spectralinstance
2022-08-01 08:33:11,283 - root - INFO - batch_size 16
2022-08-01 08:33:11,283 - root - INFO - img_size [512, 512, 512]
2022-08-01 08:33:11,283 - root - INFO - box_size [512, 512, 512]
2022-08-01 08:33:11,283 - root - INFO - input_nc 5
2022-08-01 08:33:11,283 - root - INFO - output_nc 5
2022-08-01 08:33:11,283 - root - INFO - netG tsit
2022-08-01 08:33:11,283 - root - INFO - ngf 64
2022-08-01 08:33:11,283 - root - INFO - init_type xavier
2022-08-01 08:33:11,283 - root - INFO - init_variance 0.02
2022-08-01 08:33:11,283 - root - INFO - z_dim 256
2022-08-01 08:33:11,283 - root - INFO - alpha 1.0
2022-08-01 08:33:11,283 - root - INFO - no_ss True
2022-08-01 08:33:11,283 - root - INFO - downsamp True
2022-08-01 08:33:11,283 - root - INFO - use_periodic_padding True
2022-08-01 08:33:11,283 - root - INFO - additive_noise False
2022-08-01 08:33:11,283 - root - INFO - nef 16
2022-08-01 08:33:11,283 - root - INFO - use_vae False
2022-08-01 08:33:11,283 - root - INFO - optimizer adam
2022-08-01 08:33:11,283 - root - INFO - beta1 0.5
2022-08-01 08:33:11,283 - root - INFO - beta2 0.999
2022-08-01 08:33:11,283 - root - INFO - lr 0.0002
2022-08-01 08:33:11,283 - root - INFO - D_steps_per_G 1
2022-08-01 08:33:11,283 - root - INFO - ndf 64
2022-08-01 08:33:11,283 - root - INFO - lambda_feat 0.5
2022-08-01 08:33:11,283 - root - INFO - no_ganFeat_loss False
2022-08-01 08:33:11,283 - root - INFO - gan_mode hinge
2022-08-01 08:33:11,283 - root - INFO - netD multiscale
2022-08-01 08:33:11,283 - root - INFO - num_D 2
2022-08-01 08:33:11,283 - root - INFO - n_layers_D 4
2022-08-01 08:33:11,283 - root - INFO - no_TTUR True
2022-08-01 08:33:11,283 - root - INFO - lambda_kld 0.05
2022-08-01 08:33:11,283 - root - INFO - cat_inp False
2022-08-01 08:33:11,283 - root - INFO - entity weatherbenching
2022-08-01 08:33:11,283 - root - INFO - project Nyx-TSIT
2022-08-01 08:33:11,283 - root - INFO - log_to_wandb True
2022-08-01 08:33:11,283 - root - INFO - log_to_screen True
2022-08-01 08:33:11,283 - root - INFO - save_checkpoint True
2022-08-01 08:33:11,283 - root - INFO - weight_init ordereddict([('conv_init', 'normal'), ('conv_scale', 0.02), ('conv_bias', 0.0)])
2022-08-01 08:33:11,283 - root - INFO - lambda_rho 0
2022-08-01 08:33:11,283 - root - INFO - full_scale True
2022-08-01 08:33:11,283 - root - INFO - global_batch_size 64
2022-08-01 08:33:11,283 - root - INFO - base_batch_size 64
2022-08-01 08:33:11,283 - root - INFO - num_epochs 60
2022-08-01 08:33:11,283 - root - INFO - enable_amp False
2022-08-01 08:33:11,283 - root - INFO - enable_apex False
2022-08-01 08:33:11,283 - root - INFO - enable_benchy False
2022-08-01 08:33:11,283 - root - INFO - enable_jit False
2022-08-01 08:33:11,283 - root - INFO - expdir /pscratch/sd/c/cjacobus/Nyx_TSIT_train
2022-08-01 08:33:11,283 - root - INFO - lr_schedule ordereddict([('scaling', 'sqrt'), ('start_lr', 0.0002), ('end_lr', 0.0), ('warmup_steps', 128)])
2022-08-01 08:33:11,283 - root - INFO - data_loader_config lowmem
2022-08-01 08:33:11,284 - root - INFO - N_out_channels 5
2022-08-01 08:33:11,284 - root - INFO - N_in_channels 5
2022-08-01 08:33:11,284 - root - INFO - use_cache None
2022-08-01 08:33:11,284 - root - INFO - ---------------------------------------------------
log_to_wandb -------------------------------------------------------------------
dir exists: /pscratch/sd/c/cjacobus/tsit/expts/tighttrain_up5_tanh_L1_spec/checkpoints/
2022-08-01 08:33:11,285 - root - INFO - rank 1, begin data loader init
2022-08-01 08:33:11,285 - root - INFO - rank 2, begin data loader init
2022-08-01 08:33:11,285 - root - INFO - rank 3, begin data loader init
2022-08-01 08:33:11,285 - root - INFO - rank 1, data loader initialized
2022-08-01 08:33:11,285 - root - INFO - rank 2, data loader initialized
2022-08-01 08:33:11,285 - root - INFO - rank 3, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 12, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 15, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 14, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 13, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 15, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 12, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 14, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 13, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 4, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 7, begin data loader init
2022-08-01 08:33:11,286 - root - INFO - rank 4, data loader initialized
2022-08-01 08:33:11,286 - root - INFO - rank 5, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 6, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 7, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 5, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 9, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 6, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 8, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 11, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 10, begin data loader init
2022-08-01 08:33:11,287 - root - INFO - rank 9, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 8, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 10, data loader initialized
2022-08-01 08:33:11,287 - root - INFO - rank 11, data loader initialized
wandb: Currently logged in as: cjacobus (use `wandb login --relogin` to force relogin)
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [TSITGenerator] was created. Total number of parameters: 991.3 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Network [MultiscaleDiscriminator] was created. Total number of parameters: 22.1 million. To see the architecture, do print(network).
Problem at: /global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py 96 build_and_launch
wandb: ERROR Error communicating with wandb process
wandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))
wandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))
wandb: ERROR For more info see: https://docs.wandb.ai/library/init#init-start-error
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 96, in build_and_launch
    wandb.init(config=self.params.params, name=self.params.name, project=self.params.project, entity=self.params.entity, resume=self.params.resuming, dir=self.params.experiment_dir)
  File "/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 996, in init
    run = wi.init()
  File "/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 651, in init
    raise UsageError(error_message)
wandb.errors.UsageError: Error communicating with wandb process
try: wandb.init(settings=wandb.Settings(start_method='fork'))
or:  wandb.init(settings=wandb.Settings(start_method='thread'))
For more info see: https://docs.wandb.ai/library/init#init-start-error
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [8] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [10] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [11] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [4] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [7] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [5] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [6] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [9] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [12] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [13] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [14] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [15] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [2] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [3] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    trainer.build_and_launch()
  File "/global/u1/c/cjacobus/Nyx-TSIT/trainers/pix2pix_trainer.py", line 164, in build_and_launch
    self.pix2pix_model = Pix2PixModel(self.params, dist.is_initialized(), self.local_rank, self.device)
  File "/global/u1/c/cjacobus/Nyx-TSIT/models/pix2pix_model.py", line 29, in __init__
    self.netG = DistributedDataParallel(self.netG, **ddp_args, find_unused_parameters=True)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
srun: error: nid002141: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=2859763.0
slurmstepd: error: *** STEP 2859763.0 ON nid002141 CANCELLED AT 2022-08-01T08:33:47 ***
srun: error: nid002212: task 12: Terminated
srun: error: nid002208: task 5: Terminated
srun: error: nid002208: task 6: Exited with exit code 1
srun: error: nid002209: task 8: Terminated
srun: error: nid002141: tasks 1-2: Exited with exit code 1
srun: error: nid002141: task 3: Terminated
srun: error: nid002208: tasks 4,7: Exited with exit code 1
srun: error: nid002212: tasks 13-15: Exited with exit code 1
srun: error: nid002209: tasks 9-11: Exited with exit code 1
srun: Force Terminated StepId=2859763.0
